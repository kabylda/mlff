workdir: first_experiment  # Working directory. Checkpoints and hyperparameters are saved there.
data:
  filepath: null  # Path to the data file. Either ASE digestible or .npz with appropriate column names are supported.
  energy_unit: eV  # Energy unit.
  length_unit: Angstrom  # Length unit.
  electric_charge_unit: e  # Electric charge unit.
  dipole_vec_unit: e * Angstrom  # Dipole vector unit.
  shift_mode: null  # Options are null, mean, custom.
  energy_shifts: null  # Energy shifts in eV to subtract.
  split_seed: 0  # Seed using for splitting the data into training, validation and test.
  neighbors_lr_bool: false  # Calculate long-range neighborhood indices. Required for modules like DispersionEnergy.
  neighbors_lr_cutoff: null  # Cutoff for calculating the long-range neighborhoods in Angstrom. Is converted to the
  # data set units internally. Note that it is not required to be equal to the lr_cutoff of the model.
  # E.g. one can calculate neighbors up to 50 Ang and use no long-range cutoff in the model during training at all.
  # More details see README.
  avg_num_neighbors: null  # Average number of neighbors per atom in the data set. If not set, it will be calculated from the data.
  filter:
    min_distance: 0.75  # Minimal allowed distance in Angstrom. Is converted to the data set units internally.
    max_force: 25.  # Maximal allowed force component in eV/Angstrom. Is converted to the data set units internally.
model:
  num_layers: 2  # Number of message passing layers.
  num_features: 128  # Number of invariant features.
  num_heads: 4  # Number of heads on the invariant features.
  num_features_head: 32  # Number of features per head used for the invariant features and the Euclidean variables.
  degrees:  # The degrees of spherical harmonics to use in the Euclidean variables.
    - 1
    - 2
    - 3
    - 4
  cutoff: 5.0  # Local cutoff to use.
  cutoff_fn: cosine  # Cutoff function to use.
  cutoff_lr: null  # Long-range cutoff used in the long range electrostatic and dispersion modules.
  cutoff_lr_damping: 2.0
  # It is possible to use no long range cutoff in the modules. Details see README.
  num_radial_basis_fn: 32  # Number of radial basis functions.
  radial_basis_fn: physnet  # Radial basis function to use.
  activation_fn: silu  # Activation function used in the MLPs.
  qk_non_linearity: identity  # Non-linearity to apply to the query and key vectors in the attention function.
  residual_mlp_1: false  # Use residual MLP on the invariant features after the the attention update.
  residual_mlp_2: false  # Use residual MLP on the invariant features after the the exchange block.
  message_normalization: sqrt_num_features # How to normalize the message function. Options are (identity, sqrt_num_features, avg_num_neighbors)
  layer_normalization_1: false  # Use layer normalization after first residual mlp.
  layer_normalization_2: false  # Use layer normalization after the second residual mlp.
  layers_behave_like_identity_fn_at_init: false  # The message passing layers behave like the identity function at initialization.
  output_is_zero_at_init: true  # The output of the full network is zero at initialization.
  use_charge_embed: false  # Use embedding module for total charge.
  use_spin_embed: false  # Use embedding module for number of unpaired electrons.
  energy_regression_dim: 128  # Dimension to which final features are projected, before atomic energies are calculated.
  energy_activation_fn: identity  # Activation function to use on the energy_regression_dim before atomic energies are calculated.
  energy_learn_atomic_type_scales: false
  energy_learn_atomic_type_shifts: false
  input_convention: positions  # Input convention.
  electrostatic_energy_bool: false
  electrostatic_energy_scale: 1.0
  dispersion_energy_bool: false
  dispersion_energy_cutoff_lr_damping: null
  dispersion_energy_scale: 1.0
  zbl_repulsion_bool: true
  use_final_bias_bool: true
optimizer:
  name: adam  # Name of the optimizer. See https://optax.readthedocs.io/en/latest/api.html#common-optimizers for available ones.
  optimizer_args: null
  learning_rate: 0.001  # Learning rate to use.
  learning_rate_schedule: exponential_decay  # Which learning rate schedule to use. See https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules for available ones.
  learning_rate_schedule_args:  # Arguments passed to the learning rate schedule. See https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules.
    decay_rate: 0.75
    transition_steps: 125000
  gradient_clipping: identity
  gradient_clipping_args: null
  num_of_nans_to_ignore: 0  # Number of repeated update/gradient steps that ignore NaNs before raising on error.
training:
  allow_restart: false  # Re-starting from checkpoint is allowed. This will overwrite existing checkpoints so only use if this is desired.
  num_epochs: 100  # Number of epochs.
  num_train: 950  # Number of training points to draw from data.filepath.
  num_valid: 50  # Number of validation points to draw from data.filepath.
  batch_max_num_nodes: null  # Maximal number of nodes per batch. Must be at least maximal number of atoms + 1 in the data set.
  batch_max_num_edges: null  # Maximal number of edges per batch. Must be at least maximal number of edges + 1 in the data set.
  batch_max_num_pairs: null
  # If batch_max_num_nodes and batch_max_num_edges is set to null, they will be determined from the max_num_of_graphs.
  # If they are set to values, each batch will contain as many molecular structures/graphs such none of the three values
  # batch_max_num_nodes, batch_max_num_edges and batch_max_num_of_graphs is exceeded.
  batch_max_num_graphs: 6  # Maximal number of graphs per batch.
  # Since there is one padding graph involved for an effective batch size of 5 corresponds to 6 max_num_graphs.
  eval_every_num_steps: 1000  # Number of gradient steps after which the metrics on the validation set are calculated.
  loss_weights:
    energy: 0.01  # Loss weight for the energy.
    forces: 0.99  # Loss weight for the forces.
    dipole_vec: 0.01 
    hirshfeld_ratios: 0.01
  use_robust_loss: false  # Use robust loss function.
  robust_loss_alpha: 1.99 # Alpha parameter for the robust loss function.
  model_seed: 0  # Seed used for the initialization of the model parameters.
  training_seed: 0  # Seed used for shuffling the batches during training.
  log_gradient_values: False  # Log the norm of the gradients for each set of weights.
  use_wandb: true  # Use wandb for logging.
  wandb_init_args:  # Arguments to wandb.init(). See https://docs.wandb.ai/ref/python/init. The config itself is passed as config to wandb.init().
    name: first_training_run
    project: mlff
    group: null
